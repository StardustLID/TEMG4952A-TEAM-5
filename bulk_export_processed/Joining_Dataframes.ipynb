{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"organizations_processed_1.csv\")\n",
    "df2 = pd.read_csv(\"organizations_processed_2.csv\")\n",
    "df3 = pd.read_csv(\"organizations_processed_3.csv\")\n",
    "org_df = df1.append(df2)\n",
    "org_df = org_df.append(df3)\n",
    "org_df = org_df.reset_index()\n",
    "org_df = org_df.drop(columns=[\"index\"])\n",
    "\n",
    "df1 = pd.read_csv(\"people_processed_1.csv\")\n",
    "df2 = pd.read_csv(\"people_processed_2.csv\")\n",
    "df3 = pd.read_csv(\"people_processed_3.csv\")\n",
    "df4 = pd.read_csv(\"people_processed_4.csv\")\n",
    "df5 = pd.read_csv(\"people_processed_5.csv\")\n",
    "ppl_df = df1.append(df2)\n",
    "ppl_df = ppl_df.append(df3)\n",
    "ppl_df = ppl_df.append(df4)\n",
    "ppl_df = ppl_df.append(df5)\n",
    "ppl_df = ppl_df.reset_index()\n",
    "ppl_df = ppl_df.drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_df = pd.read_csv(\"acquisitions_processed.csv\")\n",
    "fund_df = pd.read_csv(\"funds_processed.csv\") #investors' investment funds\n",
    "fund_rd_df = pd.read_csv(\"funding_rounds_processed.csv\") # each funding round in the dataset\n",
    "ipo_df = pd.read_csv(\"ipos_processed.csv\") \n",
    "csum_df = pd.read_csv(\"checksum_processed.csv\") # The sum of data present in each csv file (the number of rows in each dataframe)\n",
    "deg_df = pd.read_csv(\"degrees.csv\") #people's education background\n",
    "cate_gp_df = pd.read_csv(\"category_groups_processed.csv\") #Company name with its group\n",
    "\n",
    "df1 = pd.read_csv(\"event_appearances_processed_1.csv\")\n",
    "df2 = pd.read_csv(\"event_appearances_processed_2.csv\")\n",
    "event_app_df = df1.append(df2)\n",
    "event_app_df= event_app_df.reset_index()\n",
    "event_app_df= event_app_df.drop(columns=[\"index\"]) # Event participation details\n",
    "\n",
    "event_df= pd.read_csv(\"events_processed.csv\") #Event details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"jobs_processed_1.csv\")\n",
    "df2 = pd.read_csv(\"jobs_processed_2.csv\")\n",
    "df3 = pd.read_csv(\"jobs_processed_3.csv\")\n",
    "df4 = pd.read_csv(\"jobs_processed_4.csv\")\n",
    "df5 = pd.read_csv(\"jobs_processed_5.csv\")\n",
    "job_df = df1.append(df2)\n",
    "job_df= job_df.append(df3)\n",
    "job_df= job_df.append(df4)\n",
    "job_df= job_df.append(df5)\n",
    "job_df= job_df.reset_index()\n",
    "job_df= job_df.drop(columns=[\"index\"]) #all job and advisory roles\n",
    "\n",
    "invest_partner_df= pd.read_csv(\"investment_partners_processed.csv\") #Partners who are responsible for their firm's investments\n",
    "investor_df= pd.read_csv(\"investors_processed.csv\") #Active investors \n",
    "org_parent_df= pd.read_csv(\"org_parents_processed.csv\") #Mapping between parent organizations and subsidaries\n",
    "\n",
    "#All dataframes are now ready, org_df is the master dataframe\n",
    "org_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Naming Issues, and dropping some overlapped columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some prefixes are needed due to duplicate coulmn names with other csv\n",
    "# Some repeated columns existing info in other dataframes can be deleted\n",
    "\n",
    "acq_df = acq_df.drop(columns=['acquiree_name', 'acquiree_country_code', 'acquiree_region', 'acquiree_city', 'rank'])\n",
    "acq_df = acq_df.add_prefix('acquisitions_')\n",
    "fund_df = fund_df.add_prefix('funds_')\n",
    "ipo_df = ipo_df.add_prefix('ipo_')\n",
    "fund_rd_df = fund_rd_df.drop(columns=['rank'])\n",
    "fund_rd_df = fund_rd_df.add_prefix('fund_rd_')\n",
    "cate_gp_df = cate_gp_df.drop(columns=['name'])\n",
    "cate_gp_df = cate_gp_df.add_prefix('cat_')\n",
    "job_df = job_df.add_prefix('job_')\n",
    "org_parent_df = org_parent_df.add_prefix('parent_org_')\n",
    "\n",
    "laed_investor_df = investor_df.add_prefix('lead_investor_')\n",
    "partner_df = investor_df.add_prefix('partner_')\n",
    "investor_df = investor_df.add_prefix('investor_')\n",
    "event_df = event_df.add_prefix('event_')\n",
    "event_df = event_df.rename(columns={'event_name': 'event_names'})\n",
    "event_app_df = event_app_df.drop(columns=['uuid'])\n",
    "\n",
    "deg_df = deg_df.drop(columns=['created_at','updated_at', 'type'])\n",
    "deg_df = deg_df.add_prefix('degree_')\n",
    "ppl_df = ppl_df.drop(columns=['created_at','updated_at'])\n",
    "ppl_df = ppl_df.add_prefix('personal_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firstly, handle investor, partners and fundings. As a partner is also an investor himself, two joinings are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining Investor and Partners\n",
    "\n",
    "#investor_df = investor_df.drop(columns=['investor_name', 'investor_country_code', 'investor_region', 'investor_city'])\n",
    "\n",
    "invest_join_df = invest_partner_df.set_index('partner_uuid').join(partner_df.set_index('partner_uuid'))\n",
    "invest_join_df = invest_join_df.set_index('investor_uuid').join(investor_df.set_index('investor_uuid'))\n",
    "invest_join_df.drop(columns=['uuid'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining investor and funding rounds\n",
    "fund_rd_df = fund_rd_df.set_index('fund_rd_uuid').join(invest_join_df.set_index('funding_round_uuid'))\n",
    "\n",
    "# Joining lead investor and funding rounds\n",
    "fund_rd_df = laed_investor_df.set_index('lead_investor_uuid').join(fund_rd_df.set_index('fund_rd_lead_investor_uuids'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we handle the event. Link the event participants to the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining event and event_app\n",
    "event_join_df = event_df.set_index('event_uuid').join(event_app_df.set_index('event_uuid'))\n",
    "\n",
    "event_join_df.drop(columns =['permalink', 'created_at', 'updated_at','event_name'], inplace = True)\n",
    "\n",
    "people_event_df = event_join_df.loc[event_join_df['participant_type'] == 'person']\n",
    "org_event_df = event_join_df.loc[event_join_df['participant_type'] == 'organization']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After sorting out all dataframes that are replated to \"people entity\", we are ready to build up a large dataframe that consists of people_uuid as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining People and Degrees\n",
    "ppl_join = ppl_df.set_index('personal_uuid').join(deg_df.set_index('degree_person_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining People and People Event Participant \n",
    "ppl_join = ppl_join.join(people_event_df.set_index('participant_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add prefix to avoid overlap of column names\n",
    "ppl_join = ppl_join.add_prefix('person_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, it is time to handle organizations, put org_uuid as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Organizations and IPO\n",
    "org_join = org_df.set_index('uuid').join(ipo_df.set_index('ipo_org_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Organizations and funds\n",
    "org_join = org_join.join(fund_df.set_index('funds_entity_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Organizations and fund_rounds\n",
    "org_join = org_join.join(fund_rd_df.set_index('fund_rd_org_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Organizations and acquisitions\n",
    "org_join = org_join.join(acq_df.set_index('acquisitions_acquiree_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Organizations and categories\n",
    "org_join = org_join.join(cate_gp_df.set_index('cat_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Organizations and its parents, if any\n",
    "org_join = org_join.join(org_parent_df.set_index('parent_org_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Organizations and Organization Event Participant\n",
    "org_join = org_join.join(org_event_df.set_index('participant_uuid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at the two large dataframes (people and organization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop away emtpy columns to improve efficiency\n",
    "ppl_join.dropna(how = 'all', axis = 1, inplace = True)\n",
    "#Drop away emtpy columns to improve efficiency\n",
    "org_join.dropna(how = 'all', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_join.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_join.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop away columns that are without any use to improve efficiency\n",
    "org_join = org_join.drop(columns = ['permalink', 'created_at', 'updated_at', 'ipo_uuid', 'ipo_org_name', 'funds_uuid', \\\n",
    "                'acquisitions_uuid', 'parent_org_parent_uuid', 'ipo_stock_symbol', 'participant_name','event_venue_name', \\\n",
    "                'event_short_description', 'event_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, it is about the linkage between people and organization, through \"job\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join Job with People\n",
    "job_join = job_df.set_index('job_person_uuid').join(ppl_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop away columns that are without any use to improve efficiency\n",
    "job_join = job_join.drop(columns=['job_uuid', 'job_permalink', 'job_created_at', 'job_updated_at', 'person_participant_name',\\\n",
    "                                 'person_degree_uuid', 'person_degree_institution_uuid', 'person_degree_person_name','person_event_venue_name',\\\n",
    "                                 'person_event_short_description', 'person_event_description'])\n",
    "job_join = job_join.rename(columns={'job_person_name': 'person_name','person_event_event_roles':'person_event_roles',\\\n",
    "                                   'event_event_roles':'event_roles', 'event_names':'event_name', 'person_event_names': 'person_event_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure uuid is not lost if export to CSV in the next cell\n",
    "org_join['uuid'] = org_join.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#If necessary, export here first to avoid the runninng codes dead due to insufficient memory\n",
    "\n",
    "job_join.to_csv('job_join.csv')\n",
    "org_join.to_csv('org_join.csv')  \n",
    "\n",
    "#Refer to Joining_Dataframes_Cont'd for further running after exporting into csv files\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since memory may not be able to handle all at once, split it into multiple dataframes before joining is safer\n",
    "job_join_arr = []\n",
    "interval = int(len(job_join) / 10)\n",
    "for i in range(10):\n",
    "    job_join_part = job_join.iloc[interval*i : interval*(i+1)]\n",
    "    job_join_arr.append(job_join_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take around half hour to run this cell\n",
    "\n",
    "#Join Organization with Job\n",
    "for i in range(10):\n",
    "    org_join = org_join.set_index('uuid').join(job_join_arr[i].set_index('job_org_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop away emtpy columns to simplify the final dataframe\n",
    "org_join.dropna(how = 'all', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at the joint dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take around 15 minutes to run this cell\n",
    "\n",
    "#Let's see the info of the final DataFrame\n",
    "org_join.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The final DataFrame\n",
    "org_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Export out as CSV, the file is around 60GB\n",
    "#org_join.to_csv('org_final_joined.csv')\n",
    "\n",
    "#Use the following code to zip before exporting\n",
    "compression_opts = dict(method = 'zip', archive_name = 'org_final_joined.csv')  \n",
    "org_join.to_csv('org_final_joined.zip', compression=compression_opts) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Problem 1: Is the following items the same as organization itself? Can we come up a way to check? If yes, we can drop all duplications to simplify our df\n",
    "\n",
    "# e.g.\n",
    "#'ipo_country',vs 'fund_rd_country', vs 'country_code'\n",
    "#'ipo_region', vs 'fund_rd_region', vs 'state_code'\n",
    "#'ipo_city', vs 'fund_rd_city', vs 'city'\n",
    "#'person_personal_rank', vs 'personal_event_rank'\n",
    "#'total_funding_usd', vs 'total_funding', vs 'funds_raised_amount_usd' \n",
    "#'category_list', vs 'category_groups_list' (what should we use eaxctly......)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# Problem 2: There are multiple rows for some organization eitites, due to their multiple funding rounds, or multiple people, etc.\n",
    "#            I plan to keep here first, but we may have to come up with a way to handle this situation\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# Problem 3: The dataframe consists of a column 'Acquirer'. I chose to join using acquiriee id instead of acquirier id,\n",
    "#            due to the intuition that venture companies are more likely to be acquired than to acquire others;\n",
    "#            However, may need to find a way to handle the situation if there is really a startup acquiring others\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# Problem 4: If a startup company has a parent company, do we also need the information of its parent company in\n",
    "#            our dataframes. My answer is yes and know, coz not sure how much could its parent company affect it;\n",
    "#            however, after all we value a startup company all by itself. If we have to incorporate its parents' data,\n",
    "#            how do we valuate its parents? We may have to build up another dataframe, or else it would be too large. \n",
    "#            Also, if its parent is an acquirer, the situation will be much more complex.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# Problem 5: Now, all data are raw, many NaN data and are unprocessed, we have to preprocess all the data here at once\n",
    "#            Also, we have to extract those that are within 5 years and fintech field to be our target data.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
