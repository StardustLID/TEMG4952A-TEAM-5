{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.max_columns', None,'display.max_row', None)\n",
    "# pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "default_figsize = (15,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import category information\n",
    "df_ctg = pd.read_csv(\"category_groups_cleaned.csv\")\n",
    "df_fd_rd = pd.read_csv(\"funding_rounds_cleaned.csv\")\n",
    "df_org = pd.read_csv(\"organizations_cleaned.csv\")\n",
    "\n",
    "# df_category_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stardust ver unique keyword search\n",
    "def unique_keyword_search(df_col: pd.Series, num_selected: int) -> list:\n",
    "    '''\n",
    "    Performs unique keyword search on a dataframe's column for its most common keywords\n",
    "\n",
    "    :param pd.Series df_col: column of a pd.DataFrame (e.g. df['col'])\n",
    "    :param int num_selected: number of keywords\n",
    "    :return: list of keywords in decreasing occurrence\n",
    "    :rtype: list\n",
    "    '''\n",
    "    keywords = []\n",
    "    for entry in df_col:\n",
    "        words = entry.split(',')\n",
    "        [keywords.append(word) for word in words]\n",
    "    sorted_keywords = Counter(keywords).most_common()\n",
    "    output_keywords = []\n",
    "    for i in range(num_selected):\n",
    "        output_keywords.append(sorted_keywords[i][0])\n",
    "    return output_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stardust ver one-hot encoder V2\n",
    "def onehot_encoder_v2(df: pd.DataFrame, col_name: str, list_selected: list) -> pd.DataFrame:\n",
    "    '''\n",
    "    Performs one-hot encoding on a dataframe's column for its values with most occurrences\n",
    "\n",
    "    :param pd.DataFrame df: dataframe to be processed\n",
    "    :param str col_name: name of the encoded column\n",
    "    :param list list_selected: list of most common values\n",
    "    :return: processed dataframe\n",
    "    :rtype: pd.DataFrame\n",
    "    '''\n",
    "    for item in list_selected:\n",
    "        df[item] = np.where(df[col_name] == item, 1, 0) # whenever df[col_name] == cat replace it with 1 else 0\n",
    "    df.drop(col_name, axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_keywords = unique_keyword_search(df_org['category_list'], 30)\n",
    "df_org['categories'] = df_org['category_list']\n",
    "\n",
    "onehot_encoder_v2(df_org, 'category_list', fintech_keywords)\n",
    "df_org.head(50)\n",
    "\n",
    "org_df = df_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CSV\n",
    "df = pd.read_csv(\"event_appearances_cleaned.csv\")\n",
    "# change the cols to keep\n",
    "keep_col = ['participant_uuid', 'participant_name']\n",
    "df = df[keep_col]\n",
    "\n",
    "event_count_dict = df['participant_name'].value_counts().to_dict()\n",
    "df['event_count'] = df['participant_name'].map(event_count_dict)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.sort_values(by='event_count', axis=0, ascending=False, inplace=True)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(axis=1, labels='index', inplace=True)\n",
    "\n",
    "event_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CSV\n",
    "df = pd.read_csv(\"investment_partners_processed.csv\")\n",
    "# change the cols to keep\n",
    "keep_col = ['funding_round_uuid','investor_uuid','partner_uuid']\n",
    "df = df[keep_col]\n",
    "\n",
    "df = df.merge(df.groupby('funding_round_uuid').agg(investor_list=('partner_uuid',list)).reset_index())\n",
    "\n",
    "#for index, row in df.iterrows():\n",
    "#    row[df.columns.get_loc('investor_list')] = np.insert(row[df.columns.get_loc('investor_list')], 0, row[df.columns.get_loc('investor_uuid')], axis=0)\n",
    "    \n",
    "#df.drop(labels=['investor_uuid', 'partner_uuid'], axis=1, inplace=True)\n",
    "df.drop_duplicates(subset='funding_round_uuid', inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(axis=1, labels='index', inplace=True)\n",
    "\n",
    "partners_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stardust ver unique keyword search\n",
    "def unique_keyword_search(df_col: pd.Series, num_selected: int) -> list:\n",
    "    '''\n",
    "    Performs unique keyword search on a dataframe's column for its most common keywords\n",
    "\n",
    "    :param pd.Series df_col: column of a pd.DataFrame (e.g. df['col'])\n",
    "    :param int num_selected: number of keywords\n",
    "    :return: list of keywords in decreasing occurrence\n",
    "    :rtype: list\n",
    "    '''\n",
    "    keywords = []\n",
    "    for entry in df_col:\n",
    "        words = entry.split(',')\n",
    "        [keywords.append(word) for word in words]\n",
    "    sorted_keywords = Counter(keywords).most_common()\n",
    "    output_keywords = []\n",
    "    for i in range(num_selected):\n",
    "        output_keywords.append(sorted_keywords[i][0])\n",
    "    return output_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stardust ver one-hot encoder\n",
    "def onehot_encoder(df: pd.DataFrame, col_name: str, num_selected: int) -> pd.DataFrame:\n",
    "    '''\n",
    "    Performs one-hot encoding on a dataframe's column for its values with most occurrences\n",
    "\n",
    "    :param pd.DataFrame df: dataframe to be processed\n",
    "    :param str col_name: name of the encoded column\n",
    "    :param int num_selected: number of values with most occurrences\n",
    "    :return: processed dataframe\n",
    "    :rtype: pd.DataFrame\n",
    "    '''\n",
    "    series = df[col_name].value_counts()\n",
    "    selected_col_name = series.head(num_selected).index.tolist()\n",
    "\n",
    "    for item in selected_col_name:\n",
    "        df[col_name + \"_\" + item] = np.where(df[col_name] == item, 1, 0) # whenever df[col_name] == cat replace it with 1 else 0\n",
    "        \n",
    "    df.drop(columns=col_name, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stardust ver one-hot encoder V2\n",
    "def onehot_encoder_v2(df: pd.DataFrame, col_name: str, list_selected: list) -> pd.DataFrame:\n",
    "    '''\n",
    "    Performs one-hot encoding on a dataframe's column for its values with most occurrences\n",
    "\n",
    "    :param pd.DataFrame df: dataframe to be processed\n",
    "    :param str col_name: name of the encoded column\n",
    "    :param list list_selected: list of most common values\n",
    "    :return: processed dataframe\n",
    "    :rtype: pd.DataFrame\n",
    "    '''\n",
    "    for item in list_selected:\n",
    "        df[item] = np.where(df[col_name] == item, 1, 0) # whenever df[col_name] == cat replace it with 1 else 0\n",
    "    df.drop(col_name, axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CSV\n",
    "ppl_df = pd.read_csv(\"people_processed.csv\")\n",
    "# change the cols to keep\n",
    "# keep_col = ['uuid','gender','featured_job_organization_uuid']\n",
    "keep_col = ['uuid','gender']\n",
    "ppl_df = ppl_df[keep_col]\n",
    "# ppl_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_df = onehot_encoder(ppl_df, 'gender', 2) # male and female\n",
    "# ppl_df.info()\n",
    "\n",
    "ppl_df.reset_index(inplace=True)\n",
    "ppl_df.drop(axis=1, labels='index', inplace=True)\n",
    "\n",
    "# read the CSV\n",
    "deg_df = pd.read_csv(\"degrees_cleaned.csv\")\n",
    "# change the cols to keep\n",
    "keep_col = ['person_uuid','degree_type','subject','started_on','completed_on','is_completed']\n",
    "deg_df = deg_df[keep_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_df.drop_duplicates(inplace=True)\n",
    "\n",
    "degree_type_to_drop = (deg_df['degree_type'] == 'unknown') | (deg_df['degree_type'] == 'Unknown') | (deg_df['degree_type'] == 'Specialization') | (deg_df['degree_type'] == 'Certificate') | (deg_df['degree_type'] == 'Certification')\n",
    "deg_df = deg_df.drop(deg_df[degree_type_to_drop].index)\n",
    "deg_df = deg_df.drop(deg_df[deg_df['subject'] == 'unknown'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use keyword to search for relevant degrees\n",
    "bachelor_keyword = ['Bachelor', 'Degree', 'BS', 'BSc', 'B.S.', 'Bsc', 'B.S',\n",
    "'BENG', 'BEng', 'B.Eng.', 'Beng', 'B.Eng', 'BE', 'B.E', 'BA', 'Ba', 'B.A.', 'B.A', 'A.B.', 'AB',\n",
    "'BBA', 'B.B.A.', 'B.B.A', 'B.Tech', 'B.Tech.', 'B.Com.', 'J.D.', 'JD', 'Juris Doctor']\n",
    "\n",
    "master_keyword = ['Master', 'Postgraduate', 'Graduate', 'MPHIL', 'MPhil', 'Mphil', 'M.Phil.', 'M.phil', 'M.Phil', 'M.S',\n",
    "'MS', 'MSc', 'M.Sc', 'Msc', 'MENG', 'MEng', 'M.Eng.', 'M.eng.',\n",
    "'MA', 'M.A', 'MBA', 'M.B.A.', 'M.B.A', 'Mba', 'M.BA.', 'M.Ba.', 'LLM']\n",
    "\n",
    "phd_keyword = ['PHD', 'Phd', 'PhD', 'P.HD', 'P.Hd', 'P.hd', 'P.H.D', 'Ph.D.', 'Ph.D.', 'PhD']\n",
    "\n",
    "# categories can be formed from one or more above elementary keywords\n",
    "# df.dropna(axis=0, subset=['degree_type'], inplace=True)\n",
    "deg_df['degree_type'].fillna(value='N/A', inplace=True)\n",
    "deg_df.loc[deg_df['degree_type'].str.contains('|'.join(phd_keyword)),'degree_type']='PhD'\n",
    "deg_df.loc[deg_df['degree_type'].str.contains('|'.join(master_keyword)),'degree_type']='Master'\n",
    "deg_df.loc[deg_df['degree_type'].str.contains('|'.join(bachelor_keyword)),'degree_type']='Bachelor'\n",
    "\n",
    "degree_type = []\n",
    "\n",
    "# perform ordinal encoding: bachelor = 1, master = 2, PhD = 3, no/others = 0\n",
    "for degree in deg_df['degree_type']:\n",
    "    if degree == 'Bachelor':\n",
    "        degree_type.append(1)\n",
    "    elif degree == 'Master':\n",
    "        degree_type.append(2)\n",
    "    elif degree == 'PhD':\n",
    "        degree_type.append(3)\n",
    "    else:\n",
    "        degree_type.append(0)\n",
    "        \n",
    "deg_df['degree_type'] = degree_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_df = onehot_encoder(deg_df, 'subject', 30)\n",
    "\n",
    "deg_df.reset_index(inplace=True)\n",
    "deg_df.drop(axis=1, labels='index', inplace=True)\n",
    "\n",
    "deg_df.sort_values(by='degree_type', ascending=False, inplace=True) # PhD > Master > Bachelor\n",
    "deg_df = deg_df.drop_duplicates(subset='person_uuid', keep=\"first\") # only consider the highest degree obtained\n",
    "\n",
    "degree_date = ['started_on', 'completed_on']\n",
    "has_degree = np.where(deg_df['degree_type'] != 0, 1, 0)\n",
    "\n",
    "# ignore for now\n",
    "# for col in degree_date:\n",
    "#     deg_df[col] = pd.to_datetime(deg_df[col], errors='coerce', format='%Y-%m-%d') # 'coerce' converts NaN to NaT\n",
    "#     mean = deg_df[col].mean()\n",
    "#     deg_df.loc[has_degree,col].fillna(value=mean, inplace=True)\n",
    "\n",
    "deg_df['is_completed'].fillna(value=0, inplace=True)\n",
    "deg_df['is_completed'] = deg_df['is_completed']*1\n",
    "\n",
    "deg_df.reset_index(inplace=True)\n",
    "deg_df.drop(axis=1, labels='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/UBS/lib/python3.8/site-packages/pandas/core/frame.py:4321: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n"
     ]
    }
   ],
   "source": [
    "# join people and degrees\n",
    "ppl_join = ppl_df.set_index('uuid').join(deg_df.set_index('person_uuid'))\n",
    "\n",
    "# some cols will use 0 as the fillna() value\n",
    "col_nan_to_zero = ['degree_type','is_completed']\n",
    "for col in col_nan_to_zero:\n",
    "    ppl_join[col].fillna(value=0, inplace=True)\n",
    "ppl_join.iloc[9:].fillna(value=0, inplace=True)\n",
    "\n",
    "ppl_join.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1589222 entries, 0 to 1589221\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count    Dtype \n",
      "---  ------       --------------    ----- \n",
      " 0   person_uuid  1589222 non-null  object\n",
      " 1   org_uuid     1589222 non-null  object\n",
      " 2   started_on   786367 non-null   object\n",
      " 3   ended_on     293932 non-null   object\n",
      " 4   is_current   1589222 non-null  bool  \n",
      "dtypes: bool(1), object(4)\n",
      "memory usage: 50.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# read the CSV\n",
    "df1 = pd.read_csv(\"jobs_cleaned_1.csv\")\n",
    "df2 = pd.read_csv(\"jobs_cleaned_2.csv\")\n",
    "df3 = pd.read_csv(\"jobs_cleaned_3.csv\")\n",
    "job_df = df1.append(df2)\n",
    "job_df= job_df.append(df3)\n",
    "job_df= job_df.reset_index()\n",
    "job_df= job_df.drop(columns=[\"index\"])\n",
    "\n",
    "# change the cols to keep\n",
    "keep_col = ['person_uuid','org_uuid','started_on','ended_on','is_current']\n",
    "job_df = job_df[keep_col]\n",
    "job_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_is_current = job_df['is_current'].array\n",
    "job_df.loc[job_is_current,'ended_on'] = job_df.loc[job_is_current,'ended_on'].fillna('2020-12-30')\n",
    "job_df.drop(columns='is_current', inplace=True)\n",
    "\n",
    "# find the duration of the job\n",
    "job_df['job_duration'] = pd.to_datetime(job_df['ended_on'], errors='coerce', format='%Y-%m-%d') - pd.to_datetime(job_df['started_on'], errors='coerce', format='%Y-%m-%d')\n",
    "# take mean for NaN\n",
    "mean = job_df['job_duration'].mean()\n",
    "job_df['job_duration'].fillna(value=mean, inplace=True)\n",
    "# convert TimeDelta to int\n",
    "job_df['job_duration'] = job_df['job_duration'].apply(lambda x: x.days)\n",
    "\n",
    "job_df.drop(columns=['started_on','ended_on'], inplace=True)\n",
    "\n",
    "# join jobs and ppl_join (= ppl + degrees)\n",
    "job_join = job_df.set_index('person_uuid').join(ppl_join.set_index('uuid'))\n",
    "\n",
    "job_join.drop(columns=['started_on','completed_on','is_completed'], inplace=True) # may not drop in final ver\n",
    "job_join.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = job_join.groupby(['org_uuid']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CSV\n",
    "df = pd.read_csv(\"investors_processed.csv\")\n",
    "# change the cols to keep\n",
    "keep_col = ['uuid','name','investment_count','founded_on','closed_on']\n",
    "df = df[keep_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0, how='any', subset=['investment_count'], inplace=True)\n",
    "\n",
    "df.drop(df[df['investment_count'] < 100].index, inplace=True)\n",
    "\n",
    "df['investment_count'] = df['investment_count'].astype(int)\n",
    "\n",
    "df.sort_values(by='investment_count', axis=0, ascending=False, inplace=True)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(axis=1, labels='index', inplace=True)\n",
    "\n",
    "investor_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_df = pd.read_csv(\"acquisitions_processed.csv\")\n",
    "fund_df = pd.read_csv(\"funds_processed.csv\") #investors' investment funds\n",
    "fund_rd_df = pd.read_csv(\"funding_rounds.csv\") # each funding round in the dataset\n",
    "ipo_df = pd.read_csv(\"cleaned_csv/ipos_cleaned.csv\") \n",
    "org_parent_df= pd.read_csv(\"cleaned_csv/org_parents_cleaned.csv\") #Mapping between parent organizations and subsidiaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All dataframes are now ready, org_df is the master dataframe\n",
    "org_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fund_rd_df\n",
    "ppl_df = job_join.drop_duplicates(subset = [\"org_uuid\"])\n",
    "fin_tech_ppl_df = ppl_df.loc[ppl_df['org_uuid'].isin(org_df[\"uuid\"])]\n",
    "ppl_df = fin_tech_ppl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Naming Issues, and dropping some overlapped columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some prefixes are needed due to duplicate coulmn names with other csv\n",
    "# Some repeated columns existing info in other dataframes can be deleted\n",
    "\n",
    "acq_df = acq_df.drop(columns=['acquiree_name', 'acquiree_country_code', 'acquiree_region', 'acquiree_city', 'rank'])\n",
    "acq_df = acq_df.add_prefix('acquisitions_')\n",
    "fund_df = fund_df.add_prefix('funds_')\n",
    "ipo_df = ipo_df.add_prefix('ipo_')\n",
    "fund_rd_df = fund_rd_df.add_prefix('fund_rd_')\n",
    "\n",
    "org_parent_df = org_parent_df.add_prefix('parent_org_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firstly, handle investor, partners and fundings. As a partner is also an investor himself, two joinings are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Investor and Partners\n",
    "\n",
    "partner_big = partners_df.loc[partners_df['partner_uuid'].isin(investor_df[\"uuid\"])]\n",
    "length = len(partner_big)\n",
    "big_partner = []\n",
    "for i in range(length):\n",
    "    big_partner.append(1)\n",
    "partner_big['any_top_partner'] = big_partner\n",
    "\n",
    "partner_df = partners_df.loc[partners_df['investor_uuid'].isin(investor_df[\"uuid\"])]\n",
    "length = len(partner_df)\n",
    "any_partner = []\n",
    "for i in range(length):\n",
    "    any_partner.append(1)\n",
    "partner_df['any_partner'] = any_partner\n",
    "\n",
    "partner_df.drop(columns=['partner_uuid', 'investor_list'], inplace=True)\n",
    "partner_big.drop(columns=['investor_uuid', 'partner_uuid', 'investor_list'], inplace=True)\n",
    "\n",
    "partner_df = partner_df.set_index('funding_round_uuid').join(partner_big.set_index('funding_round_uuid'))\n",
    "\n",
    "partner_df.fillna(0)\n",
    "\n",
    "partner_df['funding_round_uuid'] = partner_df.index\n",
    "invest_join_df = investor_df.set_index('uuid').join(partner_df.set_index('investor_uuid'))\n",
    "invest_join_df['any_partner'] = invest_join_df['any_partner'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining investor and funding rounds\n",
    "fund_rd_df = fund_rd_df.set_index('fund_rd_uuid').join(invest_join_df.set_index('funding_round_uuid'))\n",
    "fund_rd_df = fund_rd_df.drop(columns=['fund_rd_name', 'fund_rd_rank','fund_rd_type', 'fund_rd_permalink',\\\n",
    "                                     'fund_rd_cb_url','fund_rd_created_at','fund_rd_updated_at','fund_rd_raised_amount',\\\n",
    "                                     'fund_rd_raised_amount_currency_code','fund_rd_post_money_valuation',\\\n",
    "                                     'fund_rd_post_money_valuation_currency_code','fund_rd_org_name','fund_rd_lead_investor_uuids',\\\n",
    "                                     'name','founded_on','closed_on'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_df.drop(columns=['ipo_uuid', 'ipo_country_code', 'ipo_region', 'ipo_city','ipo_stock_symbol','ipo_stock_exchange_symbol'],inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we handle the event. Link the event participants to the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining event and event_app\n",
    "#event_join_df = event_df.set_index('event_uuid').join(event_app_df.set_index('event_uuid'))\n",
    "\n",
    "#people_event_df = event_join_df.loc[event_join_df['person'] == 1]\n",
    "#org_event_df = event_join_df.loc[event_join_df['organization'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After sorting out all dataframes that are replated to \"people entity\", we are ready to build up a large dataframe that consists of people_uuid as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining People and Degrees\n",
    "#ppl_join = ppl_df.set_index('personal_uuid').join(deg_df.set_index('degree_person_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining People and People Event Participant \n",
    "#ppl_join = ppl_join.join(people_event_df.set_index('participant_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add prefix to avoid overlap of column names\n",
    "#ppl_join = ppl_join.add_prefix('person_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, it is time to handle organizations, put org_uuid as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Organizations and fund_rounds\n",
    "org_join = org_df.set_index('uuid').join(fund_rd_df.set_index('fund_rd_org_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Organizations and IPO\n",
    "org_join = org_join.join(ipo_df.set_index('ipo_org_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Organizations and funds\n",
    "org_join = org_join.join(fund_df.set_index('funds_entity_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Organizations and acquisitions\n",
    "org_join = org_join.join(acq_df.set_index('acquisitions_acquiree_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Organizations and categories\n",
    "#org_join = org_join.join(cate_gp_df.set_index('cat_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Organizations and its parents, if any\n",
    "org_join = org_join.join(org_parent_df.set_index('parent_org_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Organizations and Organization Event Participant\n",
    "org_join = org_join.join(event_df.set_index('participant_uuid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at the current large dataframes and drop some columns (org_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop away emtpy columns to improve efficiency\n",
    "ppl_join.dropna(how = 'all', axis = 1, inplace = True)\n",
    "#Drop away emtpy columns to improve efficiency\n",
    "org_join.dropna(how = 'all', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_join.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop away columns that are without any use to improve efficiency\n",
    "org_join = org_join.drop(columns = ['fund_rd_state_code', 'fund_rd_region', 'fund_rd_city', \\\n",
    "                'acquisitions_uuid', 'parent_org_rank', 'participant_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, it is about the linkage between people and organization, through \"job\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join Job with People\n",
    "#job_join = job_df.set_index('job_person_uuid').join(ppl_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop away columns that are without any use to improve efficiency\n",
    "#job_join = job_join.drop(columns=['job_uuid', 'person_participant_name',\\\n",
    "#                                 'person_degree_uuid', 'person_degree_institution_uuid', 'person_event_venue_name',\\\n",
    "#                                 'person_event_short_description', 'person_event_description'])\n",
    "#job_join = job_join.rename(columns={'job_person_name': 'person_name','person_event_event_roles':'person_event_roles',\\\n",
    "#                                   'event_event_roles':'event_roles', 'event_names':'event_name', 'person_event_names': \\\n",
    "#                                    'person_event_name', 'job_org_name': 'organization_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join Organization with Job\n",
    "org_join = org_join.join(ppl_df.set_index('org_uuid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = org_join\n",
    "df.iloc[:,:100].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop away emtpy columns to simplify the final dataframe\n",
    "org_join.dropna(how = 'all', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final preparation for the overall joint dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the uuid from index and put back into a proper column\n",
    "org_join['uuid'] = org_join.index\n",
    "x = org_join.columns.get_loc('ipo_org_name')\n",
    "#Move the uuid column and the organization name column to the front for easier references\n",
    "cols = org_join.columns.tolist()\n",
    "cols = cols[-1:] + cols[x:x+1] + cols[0:x] + cols[x+1:-1]\n",
    "org_join = org_join[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reindex the dataframe so that the index column is not uuid any more, but integer values \"0, 1, 2...\"\n",
    "new_index = []\n",
    "for i in range(len(org_join)):\n",
    "    new_index.append(i)\n",
    "org_join.index = new_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at the joint dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The final DataFrame\n",
    "org_join = org_join.rename(columns={\"ipo_org_name\": \"company_name\",\"uuid\": \"company_uuid\" })\n",
    "org_join.iloc[50000:50004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_join.info()\n",
    "unique_df = org_join.drop_duplicates(subset = [\"company_uuid\"])\n",
    "unique_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_join.to_csv(unifed_csv_with_multiple_funds, index=False)\n",
    "uniqie_df.to_csv(unifed_csv_without_duplicated_company, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#df.info() for all columns\n",
    "num_col = len(org_join.columns)\n",
    "num_col_100 = int(num_col/100)\n",
    "for i in range (1,num_col_100):\n",
    "    print(org_join.iloc[:,100*(i-1):100*i].info())\n",
    "print(org_join.iloc[:,num_col_100:].info())\n",
    "\n",
    "#import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "\n",
    "unique_df = org_join.drop_duplicates(subset = [\"uuid\"])\n",
    "\n",
    "\n",
    "founded_on = pd.to_datetime(unique_df['founded_on'], infer_datetime_format=True)\n",
    "today = ['1/1/2021']\n",
    "today = pd.DataFrame(today)\n",
    "today = pd.to_datetime(today[0], infer_datetime_format=True)\n",
    "today = today.apply(lambda x: x.value)\n",
    "today = today.loc[0]\n",
    "\n",
    "#Convert from milliseconds and microseconds into unit of year\n",
    "founded_on = founded_on.apply(lambda x: x.value)\n",
    "age = founded_on.apply(lambda x: (today-x)/(365*24*3600*1000*1000000))\n",
    "age = round(age, 2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
